
title: 爬虫_遍历爬取exploit-db 
date: 2016-10-05 18:49:58
tags: spider 
---
exploit-db站点提供了搜素查询,但是有验证码放爬虫,好在网站还提供了展示窗口,直接一层一层爬取解析数据就行
![](http://7xpyfe.com1.z0.glb.clouddn.com/exploit-db.png)

分页爬取第一级页面,这些页面通过next字段判断是否继续爬取下一页,然后在对爬取下来的二级页面解析,爬取,获得我们需要的三级页面,然后进行解析数据.
这里二级页面和三级页面数量太大,爬取下来的html以文件形式存在,使用进程操作的时候,我把这些文件移动到bak目录,这样进程之间会先查找文件是否存在,不存在说明已经被其他进程操作了,便会接着操作存在的文件.避免了无用功.

<!-- more-->


### 代码区

```python

import os
from bs4 import BeautifulSoup
import sys
import traceback
import os.path as OP
import time
import pdb
from multiprocessing import Process
import random
import os.path as OP


PROCESS_NUM = 20

HOME_URL=['https://www.exploit-db.com/remote/','https://www.exploit-db.com/webapps/','https://www.exploit-db.com/local/','https://www.exploit-db.com/dos/','https://www.exploit-db.com/shellcode/']
ARRS_TD_CLASS=['description']
ARRS_TABLE_CLASS=['exploit_list']

def parser_ch(pid):
    ofile = open('log/%d.log'%pid, 'a')
    files_list= os.listdir('spider_tmp/')
    for file_name in files_list:
	fp =open("cve_list","a")
        path= 'spider_tmp/'+file_name
        soup = BeautifulSoup(open(path,'r'),'html5lib')
        for tag_table in soup.find_all('table'):
            arrs_class = tag_table.get('class')
            if  arrs_class == ARRS_TABLE_CLASS:
                    tag_tr = tag_table.tr
		    tdlist=tag_tr.find_all('td')
		    if  tdlist[2].a :
			str_tmp=tdlist[2].a.string.strip()
			#if "..." not in str_tmp and "N/A" not in str_tmp :
			if "..." not in str_tmp :
			    #print str_tmp
			    fp.write(str_tmp+'\n')
	if OP.exists(path):
	    cmd1='mv spider_tmp/%s  ch_data_bak/' % file_name
	    os.system(cmd1)
	ofile.write("parser_ch: %s\n" % file_name)
def spider_ch(arr_url,pid):
	try:
	    ofile = open('log/%d.log'%pid, 'a')
            path_month = './spider_tmp/'
            if not OP.exists(path_month):
                os.mkdir(path_month)
            url = arr_url
	    f_name =url.rsplit('/')[4]
#	    print url,f_name
            file_name = path_month + f_name
	    cmd = ["/usr/bin/curl", "-s", "-L",
                   "-A", '"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:48.0) Gecko/20100101 Firefox/48.0"',
                   "-H", '"zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3"',
                   "-o", file_name, url]
            cmdline = ' '.join(cmd)
            os.system(cmdline)
	    ofile.write("parser_ch  create file name : %s\n" % file_name)
        except:
            traceback.print_exc()

def parser_home(pid):
    ofile = open('log/parser_home.log', 'a')
    files_list= os.listdir('spider_data')
    for file_name in files_list:
	#fp =open("url","a")
        path= 'spider_data/'+file_name
	#pdb.set_trace()   			
	if OP.exists(path):
	        cmd1='mv spider_data/%s  home_data_bak/' % file_name
	        os.system(cmd1)
		num=1
		bak_path='home_data_bak/%s' %file_name
		soup = BeautifulSoup(open(bak_path,'r'),'html5lib')
		for tag_td in soup.find_all('td'):
		    arrs_class = tag_td.get('class')
		    if  arrs_class == ARRS_TD_CLASS:
			    tag_a = tag_td.a
			    tag_href= str(tag_a.get('href'))
	#		    print tag_href
			    spider_ch(tag_href,pid)
			    num+=1
			    time.sleep(5)
		#	    fp.write(tag_href+'\n')
	else:
	    ofile.write("parser_home_error: %s, alway copyed! \n" % file_name)
	ofile.write("parser_home: %s, spider_ch num = %d\n" %( file_name,num))

def spider_home():
	try:
	    pid= "spider_home"
	    ofile = open('log/%s.log'%pid, 'a')
            path_month = './spider_data/'
            if not OP.exists(path_month):
                os.mkdir(path_month)
	    for type_url in HOME_URL:
                for i in range(1,10000):
		    STAT=1	
		    astring=[]
		    name_tmp= type_url.split('/')[3]
		    name_tmp_num=i
		    file_tmp_name=name_tmp+str(name_tmp_num)
		    file_name = path_month +file_tmp_name 
		    url = type_url+'?pg=%d' % name_tmp_num
		    cmd = ["/usr/bin/curl", "-s", "-L",
			   "-A", '"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:48.0) Gecko/20100101 Firefox/48.0"',
			   "-H", '"zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3"',
			   "-o", file_name, url]
		    cmdline = ' '.join(cmd)
		    print cmdline
		    ofile.write("spider_home: %s\n" % file_name)
		    os.system(cmdline)
		    time.sleep(5)	
        	    soup = BeautifulSoup(open(file_name,'r'),'html5lib')
		    #pdb.set_trace()   			
		    for alist in  soup.find_all('a' ,attrs={"class": "color"}):
			astring.append(alist.string)
		    print astring
		    if "next" not in astring :
				STAT=0
		    if STAT==0:
			break
		    del cmd
		    del astring
        except:
            traceback.print_exc()
def worker(pid):
    parser_home(pid)
    #parser_ch(pid)
def main():
    ps = [Process(target=worker, args=(pid,)) for pid in xrange(PROCESS_NUM)]
    for p in ps:
        time.sleep(3)
        p.start()
    print "waiting subprocess to exit..."
    for p in ps:
        p.join()

if __name__ == '__main__':
    if not os.path.exists('log'):
        os.mkdir('log')
    if not os.path.exists('home_data_bak'):
        os.mkdir('home_data_bak')
   # spider_home()
    main()


```

